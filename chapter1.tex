\section{Introduction}
\label{chap:Introduction}

\subsection{Background}

In-context learning (ICL) is an emerging technique for rapidly adapting large language models (LLMs), i.e., GPT-4 \cite{achiam2023gpt} and LLaMA2 \cite{touvron2023llama}, to new tasks without fine-tuning the pre-trained parameters \cite{brown2020language}. The key idea behind ICL is to provide LLMs with labeled examples as in-context demonstrations (demos) within the prompt context before a test query. Through learning from the demos, LLMs are able to generate responses to queries based on ICL \cite{dong2022survey,min2022rethinking}.

Several existing works, however, have demonstrated the highly unstable nature of ICL \cite{zhao2021calibrate,chen2022relation}. Specifically, performance on target tasks using ICL can vary wildly based on the selection and order of demos, giving rise to highly volatile outcomes ranging from random to near state-of-the-art (SOTA) \cite{qiang2020toward,lu2021fantastically, min2022rethinking,pezeshkpour2023large,qiang2024prompt}. Correspondingly, several approaches \cite{liu2021makes,wu2022self,nguyen2023context} have been proposed to address the unstable issue of ICL. 

Further research has looked at how adversarial examples can undermine the performance of ICL \cite{zhu2023promptbench,wang2023adversarial,wang2023robustness,shayegani2023survey}. These studies show that maliciously designed examples injected into the prompt instructions \cite{zhu2023promptbench,zou2023universal,xu2023instructions}, demos \cite{wang2023adversarial,mo2023trustworthy}, or queries \cite{wang2023robustness,kandpal2023backdoor} can successfully attack LLMs to degrade their performance, revealing the significant vulnerabilities of ICL against adversarial inputs. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{fig/example.png} 
  \caption{Illustrations of ICL using clean prompt and adversarial prompt. Given the clean in-context demos, LLMs can correctly generate the sentiment of the test queries. The previous attacks \cite{wang2023adversarial} at the character level involve minor edits in some words, such as altering `so' to `s0' and `film' to `fi1m', of these in-context demos, leading to incorrect sentiment generated for the test queries. However, ours \textbf{learns} to \textbf{append} adversarial suffixes like `For' and `Location' to the in-context demos to efficiently and effectively \textbf{hijack} LLMs to generate the \textbf{unwanted target}, e.g., the `negative' sentiment, \textbf{regardless} of the test query content. \label{fig:example}}
\end{figure*}

While existing adversarial attacks have been applied to evaluate LLM robustness, they have some limitations in practice. Most character-level attacks, e.g., TextAttack \cite{morris2020textattack} and TextBugger \cite{li2018textbugger}, can be easily detected and evaded through grammar checks, limiting real-world effectiveness \cite{qiang2022tiny,jain2023baseline}. Some other attacks like BERTAttack \cite{li2020bert} require an extra model to generate adversarial examples, which may not be feasible in real-world applications. Crucially, existing attacks are not specifically crafted to target techniques based on LLMs, i.e., ICL. As such, the inherent security risks of LLMs remain largely unexplored. There is an urgent need for red teaming tailored to ICL to expose the substantial risk of LLMs for further evaluating their adversarial robustness against potential real-world threats.

\subsection{Adversarial Attack on ICL}
This work proposes a novel adversarial attack specifically targeting ICL. We develop a gradient-based prompt search algorithm to learn adversarial suffixes in order to efficiently and effectively hijack LLMs via adversarial ICL. \cite{zhu2023promptbench,wang2023robustness} are the closest works to ours where they `search' adversarial examples to simply manipulate model outputs. Yet, our attack method `learns' adversarial tokens that directly hijack LLMs to generate the unwanted target that disrupts alignment with the desired output. In Figure \ref{fig:example}, we illustrate the major difference between the previous attack and the proposed attack: ours hijacks LLMs to output the unwanted target response (e.g.,`negative') regardless of the query content. Furthermore, instead of manipulating the prompt instructions \cite{zhu2023promptbench,zou2023universal}, demos \cite{wang2023adversarial,mo2023trustworthy}, or queries \cite{wang2023robustness,kandpal2023backdoor} leveraging standard adversarial examples, e.g., character-level attacks \cite{morris2020textattack,li2018textbugger}, which are detectable easily, our hijacking attack is imperceptible in that it adds only 1-2 suffixes to the demos, as shown in Figure \ref{fig:example}. Specifically, these suffixes are semantically incongruous but not easily identified as typos or gibberish compared to the existing ICL attack \cite{wang2023adversarial}. Finally, the backdoor attack during ICL \cite{kandpal2023backdoor} requires a trigger, which is impractical in real-world scenarios, whereas our attack hijacks the LLM to generate the unwanted target without triggering the queries.

Our extensive experiments validate the efficacy of our hijacking attacks. First, the attacks reliably induce LLMs to generate the targeted and misaligned output from the desired ones. Second, the adversarial suffixes learned via gradient optimization are transferable, remaining effective on different demo sets. Third, the suffix transferability holds even across different datasets for the same downstream task. Finally, our analysis shows that the adversarial suffixes distract LLMs' attention away from the task-relevant concepts. Our adversarial ICL attack poses a considerable threat to practical LLM applications due to its robust transferability and imperceptibility.

\subsection{Defense against Adversarial Attack on ICL}

As this work represents one of the first efficient adversarial demonstration attacks during ICL, strategies for defending against such attacks have yet to be thoroughly investigated. Recently, \cite{mo2023test} introduced a method for defending against back-door attacks at test time, leveraging few-shot demonstrations to correct the inference behavior of poisoned LLMs. Similarly, \cite{wei2023jailbreak} explored the power of in-context demos in manipulating the alignment ability of LLMs and proposed in-context attack and in-context defense methods for jailbreaking and guarding the aligned LLMs. Consequently, we explore the potential of using in-context demos exclusively to rectify the behavior of LLMs subjected to our hijacking attacks. Our defense strategy employs additional clean in-context demos at test time to safeguard LLMs from being hijacked by adversarial in-context demos. The experimental results demonstrate the efficacy of our proposed defense method against various adversarial demonstration attacks. 

\subsection{Contribution}
This work makes the following original contributions: (1) We propose a novel stealthy adversarial attack targeting in-context demos to hijack LLMs to generate unwanted target output during ICL. (2) We design a novel gradient-based prompt search algorithm to learn and append adversarial suffixes to the in-context demos. (3) Our extensive experiments demonstrate the effectiveness and transferability of the proposed adversarial ICL attack across various demo sets, LLMs, and tasks. (4) The proposed test time defense strategy effectively protects LLMs from being compromised by adversarial demonstration attacks.  