\section{The Defense Method} 
\label{chap:defense}

Having developed the ICL hijacking attack by incorporating adversarial tokens into the in-context demos, we now present a straightforward yet potent defense strategy to counter this attack. Initially, we assume that defenders treat LLMs as black-box, lacking any insight into their training processes or underlying parameters. The defenders apply defense on the input prompt $p$ directly during test-time evaluation. Their goal is to rectify the behavior of LLMs and induce LLMs to generate desired responses to user queries. 

Given an input prompt $p^\prime$ that includes adversarial tokens within the demos $C^\prime$, we assume that LLMs, when presented with demos containing clean data for the same tasks, can understand the genuine intent of the user's query through ICL, rather than being misled by the adversarial demos. In this context, `clean data' refers to data without any adversarial tokens and is randomly selected from the training set. More precisely, the defenders modify the input prompt $p^\prime$ into $\Tilde{p}$ by appending or inserting more clean demos into the demo set $C^\prime$, as follows: $\Tilde{p} = [I; C^\prime; \Tilde{C}; S(x_Q,\_)]$. $\Tilde{C} = [S(\Tilde{x}_1, \Tilde{y}_1);\  \cdots; \ S(\Tilde{x}_N, \Tilde{y}_N)]$ here denotes the clean demos selected from the training set. Through this approach, the defender guarantees that the in-context demos align with the user's query and possess resilience against adversarial attacks. 
In our experiments, we maintained an equal number of demos in $C^\prime$ and $\Tilde{C}$ and observed that this method resulted in effective defense across a range of datasets and tasks. 