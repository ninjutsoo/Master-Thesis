\section{Related Work}
\label{chap:relatedwork}

\subsection{In-Context Learning}

LLMs have shown impressive performance on numerous NLP tasks \cite{devlin2018bert,lewis2019bart,radford2019language}. Although fine-tuning has been a common method for adapting models to new tasks, it is often less feasible to fine-tune extremely large models with over 10 billion parameters. As an alternative, recent work has proposed ICL, where the model adapts to new tasks solely via inference conditioned on the provided in-context demos, without any gradient updates \cite{brown2020language}. By learning from the prompt context, ICL allows leveraging massive LLMs' knowledge without the costly fine-tuning process, showcasing an exemplar of the LLMs' emergent abilities \cite{schaeffer2023emergent,wei2022emergent}.

Intensive research has been dedicated to ICL. Initial works attempt to find better ways to select labeled examples for the demos \cite{liu2021makes,rubin2021learning}. 
For instance, \cite{liu2021makes} presents a simple yet effective retrieval-based method that selects the most semantically similar examples as demos, leading to improved accuracy and higher stability. Follow-up works have been done to understand why ICL works \cite{xie2021explanation,razeghi2022impact,min2022rethinking,wei2023larger,kossen2023context}. \cite{xie2021explanation} provides theoretical analysis that ICL can be formalized as Bayesian inference that uses the demos to recover latent concepts. Another line of research reveals the brittleness and instability of ICL approaches: small changes to the demo examples, labels, or order can significantly impact performance \cite{lu2021fantastically,zhao2021calibrate,min2022rethinking,nguyen2023context}.

\subsection{Adversarial Attacks on LLMs}

Early adversarial attacks on LLMs apply simple character or token operations to trigger the LLMs to generate incorrect predictions, such as TextAttack \cite{morris2020textattack} and BERT-Attack \cite{li2020bert}.
Since these attacks usually generate misspelled and/or gibberish prompts that can be detected using spell checker and perplexity-based filters, they are easy to block in real-world applications. Some other attacks struggled with optimizing over discrete text, leading to the manual or semi-automated discovery of vulnerabilities through trial-and-error \cite{li2021improving, perez2022ignore,li2023learning,qiang2023interpretability,casper2023explore,kang2023exploiting,li2023multi,shen2023anything}. For example, jailbreaking prompts are intentionally designed to bypass an LLM’s built-in safeguard, eliciting it to generate harmful content that violates the usage policy set by the LLM vendor \cite{shen2023anything,zhu2023autodan,chao2023jailbreaking,mehrotra2023tree,jeong2023hijacking,guo2024cold,yu2024don}. These red teaming efforts craft malicious prompts in order to understand LLM’s attack surface \cite{ganguli2022red}. However, the discrete nature of text has significantly impeded learning more effective adversarial attacks against LLMs. 

Recent work has developed gradient-based optimizers for efficient text modality attacks. For example, \cite{wen2023hard} presented a gradient-based discrete optimizer that is suitable for attacking the text pipeline of CLIP, efficiently bypassing the safeguards in the commercial platform. \cite{zou2023universal}, building on \cite{shin2020autoprompt}, described an optimizer that combines gradient guidance with random search to craft adversarial strings that induce LLMs to respond to the questions that would otherwise be banned. Subsequently, \cite{schwarzschild2024rethinking} utilized the optimization algorithm from \cite{zou2023universal} to identify the minimal prompt that elicits the target output, which may reveal private or copyrighted content, in the zero-shot setting of ICL. This method could also circumvent certain safeguards, such as in-context unlearning \cite{pawelczyk2023context}, to provoke harmful responses. More recently, \cite{zhao2024universal} proposed poisoning demonstration examples and prompts to make LLMs behave in alignment with pre-defined intentions. 

Our hijacking attack algorithm falls into this stream of work, yet we target few-shot ICL instead of zero-shot queries. We use gradient-based prompt search to automatically learn effective adversarial suffixes rather than manually engineered prompts. Importantly, we show that LLMs can be hijacked to output the targeted unwanted output by appending optimized adversarial tokens to the ICL demos, which reveals a new lens of LLM vulnerabilities that may have been missed by prior approaches. 

\vspace{-0.2cm}
\subsection{Defense Against Attacks on LLMs}
\vspace{-0.2cm}

The existing literature on the robustness of LLMs includes various strategies for defense \cite{goyal2023survey,studnia2023evaluating,liu2023shortcuts,xu2024llm,wu2024new}. However, most of these defenses, such as those involving adversarial training \cite{liu2020adversarial,li2023defending,formento2024semrode,wang2024mitigating} or data augmentation \cite{qiang2024prompt,yuan2024rigorllm}, need to re-train or fine-tune the models, which is computationally infeasible for LLM users. Moreover, the restriction of many closed-source LLMs to only permit query access for candidate defenses introduces new challenges. 

Recent studies focus on developing defenses against attacks on LLMs that utilize adversarial prompting. \cite{jain2023baseline} and \cite{alon2023detecting} have suggested the use of perplexity filters to detect adversarial prompts. While the filters are effective at catching the attack strings that contain gibberish words or character-level adversarial tokens with high perplexity scores, they fall short in detecting more subtle adversarial prompts, like the ones used in our adversarial demonstration attacks with as low perplexity as clean samples shown in Figure \ref{fig:ppl}. Recently, \cite{mo2023test} introduced a method to mitigate backdoor attacks at test time by identifying the task and retrieving relevant defensive demonstrations. These demonstrations are combined with user queries to counteract the adverse effects of triggers present in backdoor attacks. This defense strategy eliminates the need for modifications or tuning of LLMs. Its objective is to re-calibrate and correct the behavior of LLMs during test-time evaluations. Similarly, \cite{wei2023jailbreak} investigated the role of in-context demonstrations in enhancing the robustness of LLMs and highlighted their effectiveness in defending against jailbreaking attacks. The authors developed an in-context defense strategy that constructs a safe context to caution the model against generating any harmful content.

So far, defense mechanisms against adversarial demonstration attacks have not been extensively explored. Our approach introduces a test-time defense strategy that uses additional clean in-context demos to safeguard LLMs from adversarial in-context manipulations. In line with prior works \cite{mo2023test,wei2023jailbreak,wang2024mitigating}, this defense strategy avoids the necessity for retraining or fine-tuning LLMs. Instead, it focuses on re-calibrating and correcting the behavior of LLMs during evaluations at test time.